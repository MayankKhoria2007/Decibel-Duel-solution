#DATASETS

The datasets have been loaded from kaggle via the kaggle api and hv been used in both the tasks.The labels have been ordinals encoded for audio classification and onehot encoded for audio generation.


#TASK 1- AUDIO CLASSIFICATION USING MEL SPECTROGRAM AND CNN

For audio classification task,I have converted the audio samples into mel spectrograms.In mel spectrograms,frequency of sound is plotted against time and each point represents the intensiy of sound of a particular frequency at a particular time.For recording frequency,frequency bins and time samples have been created.The frequency domain is converted to mel domain  by multiplying the linear spectrogram(generated by applying torchaudio.load on the waveform) with a mel filterbank mtrix whuch is computed internally. The mel spectrogram is generated in the form of a 4D tensor in our code where the first dimension is the number of audio samples,the second dimension is the no. of channels of audio,the third dimension is the number of frequncy bins and the fourth dimension is the no. of time samples.Mel spectrograms are used for this task as they show a lot of details about the audio which are needed which for classification.The generated tensor is passed on to a cnn(convolutional neural network).Here audio samples have also been divided into batches of size 32 for better computational efficiency while training.Also the train test split technique has been used for model evaluation.


In the cnn first we have convolutional layers which extract features like patterns in loudness and pitch from the spectrogram.These layers apply filters to the mel spectrogram tensor .We keep on increasing the no. of channels in each layer so that we can extract more features.These filters are in the form of matrix which contains some weights.This matrix moves across the tensor and feature are extracted and we get a feature map as the output of a convolutonal layer.The later convolutional layers extract more deeper features.The final output after applying all convolutional layers is resized into a 2D tensor so that it can be passed to the fully connected layers of the neural network which require a 2D tensor as input.After passing through the fully connected layers,the model outputs a logit score for each class for all samples of the batch.We use cross entropy loss as the loss function here as it is muti class classifcation problem.The gradient descent technique is used to minimize the error and the Adam optimizer has been used to update the model parameters.I have trained the model for 30 epochs.At the end of training loop,predictions are generated on the test data and they are saved to a csv file "submissions.csv".


#TASK 2-AUDIO GENERATION USING CONDITIONAL GAN

For task 2,I have used a Conditional GAN for generating audio samples of the predefined classes.
In the data preprocessing step,I hv converted the audio sampes to mel spectrograms and also applied time and frequency masking to the spectrogram which ensures that the model is not dependent only on some selected frquency and time samples and thus it prevents overfitting.

In conditional GAN,we have a Generator and a Discriminator.The Generator generates fake spectrograms based on a condition  and tries to fool the Dicriminator which dicriminates between real and fake data.As we train the model,the generator and the discriminator keep on improving and ultimately the generator generates spectrograms which are close to the real spectrograms.

##ARCHITECTURE OF GENERATOR

The generator receives a random noise vector and the one hot encoded labels as input.The latent dimensions represents the dimension of the noise vector for each sample.The noise vector and the class labels are conacatenated to a single vector.Then they are projected to a 4D vector(a random spectrogram is generated) using a linear layer so that they can be passed through the deconvolutional layers.The deconvolutional layer do the opposite of convlutional layers.They add feautures to the input spectrogram and we keep on decreasing the no. of channels.The final output of the generator is a mel spectrogram which has 1 channel and its shape is (128,512).

##ARCHITECTURE OF DISCRIMINATOR

The discriminator receives specttrograms and labels as input.It projects the labels into a 4D tensor using the linear layer of neural network so that it can be concatenated with the spectrogram.This is done so that the discriminator can identify the relationships between the spectrograms and the one hot encoded labels.Then the resultant 4D tensor(having 2 channels) is passed through convolutional layers to extract features from it and the model finally outputs a single vale which is the logit score that determines whether spectrograms are real or fake.

##OVERVIEW OF TRAINING LOOP

I have trained the model for 100 epochs.
Here we use BCEWithLogitsLoss as the loss function as we want to know from dicriminator whether spectrograms are real or fake.We define separate optimizers and losses for generator and discriminator.For dicriminator,we push the probability for real spectrograms to 1 and for fake spectrograms(generated by generator) to 0 through gradient descent.Here probability means the probability of the spectrogram being real .For generator,we push the probabilty for fake spectrograms to 1 so that the spectrograms generated by it keep on becoming more realistic.

##VOCODER

The vocoder converts spectrograms to  audio samples.Here I hv used a pretrained hifi-GAN(loaded from torch.hub) as the vocoder.The hifi-GAN generates high quality and realistic audio samples from the spectrograms.Its speed is also fast compared to other vocoders.The audio samples and spectrograms are saved after evey epoch into a file "outputs_gan".




