{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1urheDM-kVvGBqL4DEKMWU3Hvpg3GxAyP",
      "authorship_tag": "ABX9TyNHNVVNBIjNmirs0IiGO4uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayankKhoria2007/Decibel-Duel-solution/blob/main/AUDIOGENERATIONCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "gpvftSnoaSDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "ToGsuW9EizR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mayankkhoria/frequencytrain\n"
      ],
      "metadata": {
        "id": "FLeN-T1u3uWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/frequencytrain.zip -d /content/frequencytrain\n",
        "\n"
      ],
      "metadata": {
        "id": "lILWEKI54UDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N1d4OsVu4S32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# INSTALLS\n",
        "# ============================\n",
        "!pip install torch torchaudio matplotlib tqdm soundfile\n",
        "\n",
        "# ============================\n",
        "# IMPORTS\n",
        "# ============================\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram, TimeMasking, FrequencyMasking\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n"
      ],
      "metadata": {
        "id": "zEk6Sb5-bvTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6wqambBMeCGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CONFIG\n",
        "# ============================\n",
        "DATASET_TRAIN_PATH = \"/content/frequencytrain/train\"\n",
        "OUTPUT_DIR = \"/content/gan_outputs\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "N_MELS = 128\n",
        "MAX_FRAMES = 512\n",
        "LATENT_DIM = 100\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "LR = 2e-4\n",
        "SAMPLE_RATE = 22050\n",
        "SAMPLES_PER_CLASS = 2\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"audio\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"specs\"), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "ICcsSstZbyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# DATASET CLASS\n",
        "# ============================\n",
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    def __init__(self, root_dir, categories, max_frames=512, n_mels=80):\n",
        "        self.root_dir = root_dir\n",
        "        self.categories = categories\n",
        "        self.max_frames = max_frames\n",
        "        self.n_mels = n_mels\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
        "\n",
        "        for cat_name in self.categories:\n",
        "            cat_dir = os.path.join(root_dir, cat_name)\n",
        "            if not os.path.isdir(cat_dir):\n",
        "                continue\n",
        "            files = [\n",
        "                os.path.join(cat_dir, f)\n",
        "                for f in os.listdir(cat_dir)\n",
        "                if f.lower().endswith(\".wav\")\n",
        "            ]\n",
        "            for f in files:\n",
        "                self.file_list.append((f, self.class_to_idx[cat_name]))\n",
        "\n",
        "        self.mel_transform = MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_fft=1024,\n",
        "            hop_length=256,\n",
        "            n_mels=self.n_mels\n",
        "        )\n",
        "        self.time_mask = TimeMasking(time_mask_param=40)\n",
        "        self.freq_mask = FrequencyMasking(freq_mask_param=12)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "\n",
        "        if sr != SAMPLE_RATE:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
        "\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        mel = self.mel_transform(wav)\n",
        "        log_mel = torch.log1p(mel)\n",
        "\n",
        "        _, _, frames = log_mel.shape\n",
        "        if frames < self.max_frames:\n",
        "            log_mel = F.pad(log_mel, (0, self.max_frames - frames))\n",
        "        else:\n",
        "            log_mel = log_mel[:, :, :self.max_frames]\n",
        "\n",
        "        log_mel = self.freq_mask(log_mel)\n",
        "        log_mel = self.time_mask(log_mel)\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return log_mel, label_vec\n"
      ],
      "metadata": {
        "id": "saeDvERqb-MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# GENERATOR\n",
        "# ============================\n",
        "class CGAN_Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        H, W = spec_shape\n",
        "        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 16)\n",
        "        self.unflatten_shape = (256, 8, 16)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 4, 2, 1),\n",
        "            nn.BatchNorm2d(16), nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=(1, 2), stride=(1, 2)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = torch.cat([z, y], dim=1)\n",
        "        h = self.fc(h)\n",
        "        h = h.view(-1, *self.unflatten_shape)\n",
        "        return self.net(h)\n"
      ],
      "metadata": {
        "id": "CiR3d3DScDbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# DISCRIMINATOR\n",
        "# ============================\n",
        "class CGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        H, W = spec_shape\n",
        "\n",
        "        self.label_embedding = nn.Linear(num_categories, H * W)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),\n",
        "            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 1, kernel_size=(8, 32), stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y):\n",
        "        label_map = self.label_embedding(y).view(-1, 1,128,512).to(spec.device)\n",
        "        h = torch.cat([spec, label_map], dim=1)\n",
        "        logits = self.net(h)\n",
        "        return logits.view(-1, 1)\n"
      ],
      "metadata": {
        "id": "wVCzNjQvcSiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# HIFI-GAN LOADING\n",
        "# ============================\n",
        "print(\"Loading HiFi-GAN from PyTorch Hub...\")\n",
        "hifigan = torch.hub.load(\"bshall/hifigan:main\", \"hifigan\", pretrained=True)\n",
        "hifigan = hifigan.to(DEVICE).eval()\n",
        "print(\"HiFi-GAN loaded.\")\n",
        "\n",
        "def mel_to_audio_hifi(log_spec_tensor):\n",
        "    mel = torch.expm1(log_spec_tensor.squeeze(1))\n",
        "    mel = mel.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        wav = hifigan(mel)\n",
        "    return wav.cpu()\n"
      ],
      "metadata": {
        "id": "CecBKo0ucXwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# DATASET & LOADER\n",
        "# ============================\n",
        "train_categories = sorted(\n",
        "    [d for d in os.listdir(DATASET_TRAIN_PATH)\n",
        "     if os.path.isdir(os.path.join(DATASET_TRAIN_PATH, d))]\n",
        ")\n",
        "NUM_CATEGORIES = len(train_categories)\n",
        "\n",
        "print(\"Classes:\", train_categories)\n",
        "\n",
        "dataset = TrainAudioSpectrogramDataset(\n",
        "    DATASET_TRAIN_PATH,\n",
        "    train_categories,\n",
        "    max_frames=MAX_FRAMES,\n",
        "    n_mels=N_MELS\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "2s8DYk-ucgd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3bb938-f63e-4c60-d65c-e8674b48c0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['dog_bark', 'drilling', 'engine_idling', 'siren', 'street_music']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# MODELS & OPTIMIZERS\n",
        "# ============================\n",
        "generator = CGAN_Generator(LATENT_DIM, NUM_CATEGORIES).to(DEVICE)\n",
        "discriminator = CGAN_Discriminator(NUM_CATEGORIES).to(DEVICE)\n",
        "\n",
        "optG = torch.optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "optD = torch.optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "BrpdALf7cr3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# TRAINING LOOP\n",
        "# ============================\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    generator.train(); discriminator.train()\n",
        "    loop = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "\n",
        "    for real_specs, labels in loop:\n",
        "        real_specs = real_specs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        b = real_specs.size(0)\n",
        "\n",
        "        real = torch.ones(b,1, device=DEVICE)\n",
        "        fake = torch.zeros(b,1, device=DEVICE)\n",
        "\n",
        "        # --- Train D ---\n",
        "        optD.zero_grad()\n",
        "\n",
        "        real_out = discriminator(real_specs, labels)\n",
        "        loss_real = criterion(real_out, real)\n",
        "\n",
        "        z = torch.randn(b, LATENT_DIM, device=DEVICE)\n",
        "        fake_specs = generator(z, labels)\n",
        "\n",
        "        fake_out = discriminator(fake_specs.detach(), labels)\n",
        "        loss_fake = criterion(fake_out, fake)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optD.step()\n",
        "\n",
        "        # --- Train G ---\n",
        "        optG.zero_grad()\n",
        "        out = discriminator(fake_specs, labels)\n",
        "        loss_G = criterion(out, real)\n",
        "        loss_G.backward()\n",
        "        optG.step()\n",
        "\n",
        "        loop.set_postfix(D=loss_D.item(), G=loss_G.item())\n",
        "\n",
        "    # ============================\n",
        "    # GENERATE AND SAVE SAMPLES\n",
        "    # ============================\n",
        "    print(\"\\nGenerating samples...\")\n",
        "    generator.eval()\n",
        "\n",
        "    for cat_idx, cat_name in enumerate(train_categories):\n",
        "        for i in range(SAMPLES_PER_CLASS):\n",
        "            z = torch.randn(1, LATENT_DIM, device=DEVICE)\n",
        "            y = F.one_hot(torch.tensor([cat_idx]), NUM_CATEGORIES).float().to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                spec = generator(z, y)\n",
        "\n",
        "            # save spectrogram\n",
        "            spec_np = spec.squeeze().cpu().numpy()\n",
        "            plt.imshow(spec_np, aspect='auto', origin='lower')\n",
        "            plt.title(f\"{cat_name} ep{epoch} s{i}\")\n",
        "            plt.axis('off')\n",
        "            plt.savefig(f\"{OUTPUT_DIR}/specs/{cat_name}_ep{epoch}_s{i}.png\",\n",
        "                        bbox_inches='tight', pad_inches=0)\n",
        "            plt.close()\n",
        "\n",
        "            # save audio\n",
        "            wav = mel_to_audio_hifi(spec)\n",
        "            sf.write(f\"{OUTPUT_DIR}/audio/{cat_name}_ep{epoch}_s{i}.wav\",\n",
        "                     wav.squeeze().numpy(),\n",
        "                     SAMPLE_RATE)\n",
        "\n",
        "    print(f\"Epoch {epoch} done.\\n\")\n"
      ],
      "metadata": {
        "id": "Igu_sdtJdRin"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}