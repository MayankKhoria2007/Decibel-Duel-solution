{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkPN6kOggT8T4SnYCljDN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayankKhoria2007/Decibel-Duel-solution/blob/main/Audioclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJEAaG3Oxp3r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "gpvftSnoaSDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "ToGsuW9EizR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mayankkhoria/frequencytrain\n",
        "!kaggle datasets download -d mayankkhoria/frequencytest\n"
      ],
      "metadata": {
        "id": "BK7NGl36zirW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/frequencytrain.zip -d /content/frequencytrain\n",
        "!unzip /content/frequencytrain.zip -d /content/frequencytest\n",
        "\n"
      ],
      "metadata": {
        "id": "lILWEKI54UDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6PV9UPtn10Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Produces submission.csv (predictions only) + Validation Accuracy\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# =============================================================\n",
        "# Dataset paths\n",
        "# =============================================================\n",
        "\n",
        "TRAIN_DIR = \"/content/frequencytrain/train\"\n",
        "TEST_DIR  = \"/content/frequencytest/train\"\n",
        "\n",
        "# =============================================================\n",
        "# Audio settings and hyperparameters\n",
        "# =============================================================\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 2.5\n",
        "NUM_SAMPLES = int(SAMPLE_RATE * DURATION)\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# =============================================================\n",
        "# Helper functions\n",
        "# =============================================================\n",
        "def load_audio(path, sr=SAMPLE_RATE, duration=DURATION):\n",
        "    waveform, orig_sr = torchaudio.load(path)\n",
        "    if orig_sr != sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=sr)\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "    samples = waveform.shape[-1]\n",
        "    target = int(sr * duration)\n",
        "    if samples > target:\n",
        "        waveform = waveform[..., :target]\n",
        "    elif samples < target:\n",
        "        pad_amt = target - samples\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amt))\n",
        "    return waveform\n",
        "\n",
        "# =============================================================\n",
        "# Dataset class\n",
        "# =============================================================\n",
        "class AudioDatasetTorch(Dataset):\n",
        "    def __init__(self, file_label_list, encoder=None, is_train=True,\n",
        "                 n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH, sr=SAMPLE_RATE):\n",
        "        self.items = file_label_list\n",
        "        self.encoder = encoder\n",
        "        self.is_train = is_train\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
        "        )\n",
        "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=18)\n",
        "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=25)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.items[idx]\n",
        "        wav = load_audio(path)\n",
        "        mel = self.mel_transform(wav)\n",
        "        mel_db = self.db_transform(mel)\n",
        "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
        "        if self.is_train:\n",
        "            if random.random() < 0.5:\n",
        "                mel_db = self.freq_mask(mel_db)\n",
        "            if random.random() < 0.5:\n",
        "                mel_db = self.time_mask(mel_db)\n",
        "        if label is None:\n",
        "            return mel_db, os.path.basename(path)\n",
        "        else:\n",
        "            y = int(self.encoder.transform([label])[0])\n",
        "            return mel_db, y\n",
        "\n",
        "# =============================================================\n",
        "# Data preparation\n",
        "# =============================================================\n",
        "labels = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n",
        "print(\"Labels:\", labels)\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "\n",
        "train_files = []\n",
        "for lab in labels:\n",
        "    folder = os.path.join(TRAIN_DIR, lab)\n",
        "    for f in os.listdir(folder):\n",
        "        if f.lower().endswith((\".wav\", \".flac\", \".mp3\")):\n",
        "            train_files.append((os.path.join(folder, f), lab))\n",
        "\n",
        "train_paths, val_paths = train_test_split(\n",
        "    train_files, test_size=0.15, stratify=[l for (_, l) in train_files], random_state=42\n",
        ")\n",
        "\n",
        "test_files = sorted([f for f in os.listdir(TEST_DIR) if f.lower().endswith((\".wav\", \".flac\", \".mp3\"))])\n",
        "test_items = [(os.path.join(TEST_DIR, f), None) for f in test_files]\n",
        "\n",
        "print(f\"Train samples: {len(train_paths)}, Val samples: {len(val_paths)}, Test samples: {len(test_items)}\")\n",
        "\n",
        "train_dataset = AudioDatasetTorch(train_paths, encoder=encoder, is_train=True)\n",
        "val_dataset   = AudioDatasetTorch(val_paths, encoder=encoder, is_train=False)\n",
        "test_dataset  = AudioDatasetTorch(test_items, encoder=None, is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# =============================================================\n",
        "# Model definition\n",
        "# =============================================================\n",
        "class EEGStyleCNN(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Dropout(0.35)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = EEGStyleCNN(n_classes=len(labels)).to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# =============================================================\n",
        "# Training setup\n",
        "# =============================================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "patience = 6\n",
        "stalled = 0\n",
        "\n",
        "# =============================================================\n",
        "# Training loop\n",
        "# =============================================================\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\", leave=False)\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        pbar.set_postfix({\"loss\": f\"{running_loss / total:.4f}\", \"acc\": f\"{100*correct/total:.2f}\"})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct, val_total, val_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [val]\", leave=False):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_correct += (preds == targets).sum().item()\n",
        "            val_total += targets.size(0)\n",
        "\n",
        "    val_loss = val_loss / val_total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} -> Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% \"\n",
        "          f\"|| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc + 1e-4:\n",
        "        best_val_acc = val_acc\n",
        "        stalled = 0\n",
        "    else:\n",
        "        stalled += 1\n",
        "        if stalled >= patience:\n",
        "            print(f\"Early stopping: no improvement in val acc for {patience} epochs.\")\n",
        "            break\n",
        "\n",
        "# =============================================================\n",
        "# Final validation accuracy\n",
        "# =============================================================\n",
        "print(\"\\n==========================\")\n",
        "print(f\"‚úÖ Final Validation Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"üèÜ Best Validation Accuracy:  {best_val_acc:.2f}%\")\n",
        "print(\"==========================\\n\")\n",
        "\n",
        "# =============================================================\n",
        "# Prediction on test data\n",
        "# =============================================================\n",
        "model.eval()\n",
        "filenames, preds_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for mel_db, fname in tqdm(test_loader, desc=\"Predicting on test set\"):\n",
        "        mel_db = mel_db.to(DEVICE)\n",
        "        outputs = model(mel_db)\n",
        "        _, pred = outputs.max(1)\n",
        "        label = encoder.inverse_transform(pred.cpu().numpy())[0]\n",
        "        filenames.append(fname[0])\n",
        "        preds_labels.append(label)\n",
        "\n",
        "submission = pd.DataFrame({\"ID\": filenames, \"Class\": preds_labels})\n",
        "out_path = \"/content/submission.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(f\"‚úÖ submission.csv created at: {out_path}\")\n"
      ],
      "metadata": {
        "id": "3JdPX9cew_Fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}